---
title: "Analiza dyskryminacyjna"
author: "Adam Michalski"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Zadanie 1
Na początek wczytujemy dane:

```{r wczytanie danych o sklepach}
suppressWarnings(library(readr))
shops <- read_csv2("Sklepy4.csv", locale = locale(encoding = "Windows-1250"), col_types = cols(
  Siec = col_factor()))
summary(shops)
colSums(is.na(shops))
```

Dzielimy dane na część uczącą i testową (90% danych bierzemy do zbioru treningowego):

```{r podzial danych o sklepach na zbior uczacy i testowy}
library(caret)

set.seed(1)
train_indices <- createDataPartition(shops$Siec, p = 0.9, list = FALSE)

train_data <- shops[train_indices, ]
test_data <- shops[-train_indices, ]

cat("Class distribution in testing set:\n")
print(table(test_data$Siec))
```

Przeprowadzamy klasyfikację modelem naiwnego Bayesa w oparciu o zbiór testowy i sprawdzamy dokładność modelu na zbiorze testowym:

```{r klasyfikacja naiwnym Bayesem}
library(klaR) 
library(caret)
naive_bayes_model <- NaiveBayes(Siec ~ Dochody + Wydatek, data = train_data, usekernel = TRUE)
naive_bayes_prediction <- suppressWarnings(predict(naive_bayes_model, test_data)$class)
naive_bayes_accuracy <- mean(naive_bayes_prediction == test_data$Siec)
cat("Naive Bayes Accuracy:", naive_bayes_accuracy, "\n")

confusionMatrix(naive_bayes_prediction, test_data$Siec)
```

Dokładność modelu jest dość niska, ale lepsza od losowego wyboru sieci. Porównamy go więc z modelem lasu losowego:

```{r klasyfikacja lasem losowym}
library(randomForest)

random_forest_model <- randomForest(Siec ~ Dochody + Wydatek, data = train_data)

random_forest_prediction <- predict(random_forest_model, test_data)
random_forest_accuracy <- mean(random_forest_prediction == test_data$Siec)
cat("Random Forest Accuracy:", random_forest_accuracy, "\n")

confusionMatrix(random_forest_prediction, test_data$Siec)
```

Prawdopodobnie oba modele nie radzą sobie za dobrze ze względu na dość małą ilość danych (w poszczególnych sieciach mamy tylko od 70 do 150 danych klientów). Sprawdźmy teraz założenie o warunkowej niezależności zmiennych objaśniających:

```{r}
siec_values <- unique(train_data$Siec)
for (siec in siec_values){
  data_test <- subset(train_data, Siec == siec, select = -c(Siec))  
  test_value <- chisq.test(x=data_test, simulate.p.value = TRUE)
  cat("Result for data from Class", siec, "\n")
  print(test_value)
}
```

Na podstawie wyników testów w ramach każdej z wartości zmiennej objaśnianej Siec odrzucamy hipotezę o niezależności zmiennych objaśniających, zatem założenia naiwnego Bayesa okazuje się być błędne. Model radzi sobie jednak nawet lepiej niż las losowy bez spełnienia teoretycznych założeń (przy braku balansowania wartości Sieci w podziale model lasów losowych ma nieznacznie większą dokładność).

# Zadanie 2
Ponownie zaczynamy od wczytania danych:

```{r}
suppressWarnings(library(readr))
iris2 <- read_csv2("irysy2.csv", locale = locale(encoding = "Windows-1250"),
                   col_types = cols(Colour = col_factor(),
                                    Species = col_factor()))
iris2
```

Dzielimy dane na część uczącą i testową (90% danych bierzemy do zbioru treningowego):

```{r podzial danych o irysach na zbior uczacy i testowy}
library(caret)

set.seed(1)
train_indices <- createDataPartition(iris2$Species, p = 0.9, list = FALSE)

train_data <- iris2[train_indices, ]
test_data <- iris2[-train_indices, ]

cat("Class distribution in testing set:\n")
print(table(test_data$Species))
```
Tworzymy model zespołowy złożony z QDA na danych liczbowych oraz naiwnego Bayesa dla kategorycznej zmiennej Colour:

```{r klasyfikacja przy pomocy QDA i naiwnego Bayesa}
library(MASS)
library(klaR)
library(caret)
QDA_data <- subset(train_data, select = -c(Colour))
NB_data <- subset(train_data, select = c(Colour))

QDA_model <- qda(Species ~ ., data = QDA_data)
QDA_probs <- predict(QDA_model, test_data[, colnames(QDA_data)])$posterior
NB_model <- NaiveBayes(Species ~ Colour, data = train_data, usekernel = TRUE)
NB_probs <- predict(NB_model, test_data[, "Colour"])$posterior

model_probs <- QDA_probs * NB_probs
model_prediction <- colnames(model_probs)[max.col(model_probs)]
model_prediction <- factor(model_prediction, levels = levels(test_data$Species))
model_accuracy <- mean(model_prediction == test_data$Species)
cat("Final model Accuracy:", model_accuracy, "\n")
model_prediction
test_data$Species
confusionMatrix(model_prediction, test_data$Species)
```

Ostateczny model myli się tylko raz błędnie diagnozując irysa gatunku Versicolor jako Virginica. Ustawiając podział na 95% otrzymujemy idealny wynik na zbiorze testowym, ale zbiór ten ma wtedy zaledwie 6 elementów.
